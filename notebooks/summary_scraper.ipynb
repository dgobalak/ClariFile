{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out a try-except for a wiki url that doesn't exist\n",
    "try:\n",
    "    article = urllib.request.urlopen(f'https://en.wikipedia.org/wiki/ThisArticleShouldntExist').read()\n",
    "except HTTPError:\n",
    "    print(\"Error caught\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORD = \"Rome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Wikipedia page and read all the paragraphs\n",
    "data = urllib.request.urlopen(f'https://en.wikipedia.org/wiki/{KEYWORD}')\n",
    "article = data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all citations and unwanted text\n",
    "text = re.sub(r'\\[[0-9]*\\]', ' ', text)\n",
    "text = re.sub(r'\\s+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters and digits for calculation of weighted frequencies\n",
    "formatted_text = re.sub('[^a-zA-Z]', ' ', text )\n",
    "formatted_text = re.sub(r'\\s+', ' ', formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a list of sentences from text\n",
    "sentence_list = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted frequency for all words in formatted_text\n",
    "stopwords = stopwords.words('english')\n",
    "word_freqs = {}\n",
    "\n",
    "for word in word_tokenize(formatted_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_freqs.keys():\n",
    "            word_freqs[word] = 1\n",
    "        else:\n",
    "            word_freqs[word] += 1\n",
    "\n",
    "max_freq = max(word_freqs.values())\n",
    "\n",
    "# Make all frequencies a fraction of 1\n",
    "for word in word_freqs.keys():\n",
    "    word_freqs[word] = (word_freqs[word]/max_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating sentence frequency scores as a sum of the individual word frequencies\n",
    "MAX_SENTENCE_LEN = 30\n",
    "sentence_scores = {}\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    for word in word_tokenize(sentence.lower()):\n",
    "        if word in word_freqs.keys():\n",
    "            if len(sentence.split(' ')) < MAX_SENTENCE_LEN:\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = word_freqs[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_freqs[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the sentences with the largest frequency scores\n",
    "NUM_SENTENCES = 8\n",
    "summary_sentences = heapq.nlargest(NUM_SENTENCES, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.inc.freq_summary import MostFrequentSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function used to get text for testing purposes\n",
    "def get_text():\n",
    "    data = urllib.request.urlopen(f'https://en.wikipedia.org/wiki/Rome')\n",
    "    article = data.read()\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    text = \"\"\n",
    "    for p in paragraphs:\n",
    "        text += p.text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing TextSummary class that summarizes passed in text\n",
    "t = MostFrequentSummary(get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.get_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiSummarizer class can summarize the wikis for a list of keywords. It uses the TestSummary class to summarize each wiki\n",
    "from src.inc.wiki_summarizer import WikiSummarizer\n",
    "w1 = WikiSummarizer(keywords=[\"Basketball\", \"Soccer\"], summarizer=\"cluster\", lang='english', target=\"fr\")\n",
    "w2 = WikiSummarizer(keywords=[\"Basketball\", \"Soccer\"], summarizer=\"freq\", lang='english', target=\"ja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.get_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2.get_summaries()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b55fb9c3b2ae460387b9f24a585a06d07a1c685c2da458447f8c89d29042cf89"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
